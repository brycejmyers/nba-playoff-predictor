{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee1dfef1",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install matplotlib\n",
    "%pip install seaborn\n",
    "%pip install joblib\n",
    "%pip install scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5ba724a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Import for Models\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.preprocessing import StandardScaler # Needed for Logistic Regression\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# Import for Evaluation and Metrics\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, recall_score, f1_score,\n",
    "    classification_report, confusion_matrix, ConfusionMatrixDisplay,\n",
    "    roc_auc_score, RocCurveDisplay\n",
    ")\n",
    "\n",
    "from sklearn.base import clone\n",
    "\n",
    "# Import for Validation\n",
    "from sklearn.model_selection import TimeSeriesSplit, cross_validate, train_test_split\n",
    "from sklearn.inspection import permutation_importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "276fadfe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data from /Users/brockolson/Desktop/STAT766/Stat 766 Final Project/nba_modeling_data_silver.csv...\n",
      "Features Selected (13): ['EFG_PCT', 'FTA_RATE', 'TM_TOV_PCT_x', 'OREB_PCT', 'OPP_EFG_PCT', 'OPP_FTA_RATE', 'OPP_TOV_PCT', 'OPP_OREB_PCT', 'AST_TO', 'AST_RATIO', 'E_PACE', 'PACE_PER40', 'POSS']\n",
      "Final Data Shapes: X=(450, 13), y=(450,)\n",
      "--------------------------------------------------\n",
      "Loaded Data Shapes - X: (450, 13), y: (450,)\n",
      "--------------------------------------------------\n",
      "\n",
      "==================================================\n",
      "STARTING TIME SERIES CROSS-VALIDATION (k=5 folds)\n",
      "==================================================\n",
      "-> Evaluating RandomForestClassifier with TSCV...\n",
      "-> Evaluating GradientBoostingClassifier with TSCV...\n",
      "-> Evaluating LogisticRegression with TSCV...\n",
      "\n",
      "================================================================================\n",
      "FINAL TIME SERIES CROSS-VALIDATION RESULTS (Mean Score Â± Std Dev)\n",
      "================================================================================\n",
      "| Model | Avg. Accuracy | Avg. Precision | Avg. Recall | Avg. F1 Score | Avg. ROC-AUC |\n",
      "|---|---|---|---|---|---|\n",
      "| RandomForest | 0.7627 (Â±0.0408) | 0.7480 (Â±0.0490) | 0.8438 (Â±0.0823) | 0.7892 (Â±0.0348) | 0.8546 (Â±0.0379) |\n",
      "| GradientBoosting | 0.7920 (Â±0.0247) | 0.8208 (Â±0.0284) | 0.7773 (Â±0.0723) | 0.7959 (Â±0.0373) | 0.8753 (Â±0.0161) |\n",
      "| LogisticRegression | 0.8560 (Â±0.0259) | 0.8724 (Â±0.0401) | 0.8587 (Â±0.0751) | 0.8620 (Â±0.0282) | 0.9410 (Â±0.0187) |\n",
      "\n",
      "==================================================\n",
      "FINAL FEATURE IMPORTANCE COMPARISON\n",
      "==================================================\n",
      "Training final Random Forest model for feature comparison plot...\n",
      "Calculating Permutation Importance (AUC Drop)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/bn/0fvjr47j2t15yg29h6rg2kfw0000gn/T/ipykernel_23072/427532257.py:240: FutureWarning: \n",
      "\n",
      "Passing `palette` without assigning `hue` is deprecated and will be removed in v0.14.0. Assign the `y` variable to `hue` and set `legend=False` for the same effect.\n",
      "\n",
      "  sns.barplot(ax=axes[0], x='Importance', y='Feature', data=mdi_df, palette=\"viridis\")\n",
      "/var/folders/bn/0fvjr47j2t15yg29h6rg2kfw0000gn/T/ipykernel_23072/427532257.py:245: FutureWarning: \n",
      "\n",
      "Passing `palette` without assigning `hue` is deprecated and will be removed in v0.14.0. Assign the `y` variable to `hue` and set `legend=False` for the same effect.\n",
      "\n",
      "  sns.barplot(ax=axes[1], x='Importance', y='Feature', data=perm_df, palette=\"magma\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Random Forest Comparison visualization saved to /Users/brockolson/Desktop/STAT766/Stat 766 Final Project/visualizations/random_forest_mdi_perm_comparison.png\n",
      "Training final Logistic Regression model for coefficient analysis...\n",
      "âœ… Logistic Regression Coefficient visualization saved to /Users/brockolson/Desktop/STAT766/Stat 766 Final Project/visualizations/logreg_coefficients.png\n",
      "\n",
      "==================================================\n",
      "FINAL LOGISTIC REGRESSION TEST SET EVALUATION\n",
      "==================================================\n",
      "\n",
      "========================================\n",
      "MODEL EVALUATION METRICS\n",
      "========================================\n",
      "Accuracy:  0.8222\n",
      "Precision: 0.8200 (Low False Positives)\n",
      "Recall:    0.8542 (Low False Negatives)\n",
      "F1 Score:  0.8367\n",
      "ROC-AUC:   0.9221 (Area Under the Curve)\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.82      0.79      0.80        42\n",
      "           1       0.82      0.85      0.84        48\n",
      "\n",
      "    accuracy                           0.82        90\n",
      "   macro avg       0.82      0.82      0.82        90\n",
      "weighted avg       0.82      0.82      0.82        90\n",
      "\n",
      "âœ… Logistic Regression ROC Curve visualization saved to /Users/brockolson/Desktop/STAT766/Stat 766 Final Project/visualizations/logistic_regression_roc_curve.png\n",
      "Saving model to /Users/brockolson/Desktop/STAT766/Stat 766 Final Project/nba_playoff_forest.joblib...\n",
      "Model serialized successfully.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/bn/0fvjr47j2t15yg29h6rg2kfw0000gn/T/ipykernel_23072/427532257.py:282: FutureWarning: \n",
      "\n",
      "Passing `palette` without assigning `hue` is deprecated and will be removed in v0.14.0. Assign the `y` variable to `hue` and set `legend=False` for the same effect.\n",
      "\n",
      "  sns.barplot(x='Coefficient', y='Feature', data=coef_df, palette=\"coolwarm\")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 800x800 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# ---------------------------------------------------------\n",
    "# CONFIGURATION: PATHS AND HYPERPARAMETERS\n",
    "# ---------------------------------------------------------\n",
    "BRONZE_DIR = \"/Users/brockolson/Desktop/STAT766/Stat 766 Final Project\"\n",
    "INPUT_FILE = os.path.join(BRONZE_DIR, \"nba_modeling_data_silver.csv\")\n",
    "MODEL_OUTPUT = os.path.join(BRONZE_DIR, \"nba_playoff_forest.joblib\")\n",
    "VIS_DIR = os.path.join(BRONZE_DIR, \"visualizations\")\n",
    "os.makedirs(VIS_DIR, exist_ok=True)\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# LEAKAGE DEFINITION (UPDATED)\n",
    "# ---------------------------------------------------------\n",
    "    # Change this list name and its contents\n",
    "REQUIRED_RAW_FEATURES = [\n",
    "    'EFG_PCT', 'FTA_RATE', 'TM_TOV_PCT_x', 'OREB_PCT', \n",
    "    'OPP_EFG_PCT', 'OPP_FTA_RATE', 'OPP_TOV_PCT', \n",
    "    'OPP_OREB_PCT', 'AST_TO', 'AST_RATIO', \n",
    "    'E_PACE', 'PACE_PER40', 'POSS'\n",
    "] \n",
    "\n",
    "def load_and_sanitize(filepath):\n",
    "    \"\"\"\n",
    "    Loads the silver dataset, selects the 13 required RAW features, \n",
    "    and handles NaN alignment by filtering the full dataset once.\n",
    "    \"\"\"\n",
    "    if not os.path.exists(filepath):\n",
    "        raise FileNotFoundError(f\"Input file not found at {filepath}\")\n",
    "    \n",
    "    print(f\"Loading data from {filepath}...\")\n",
    "    df = pd.read_csv(filepath)\n",
    "    \n",
    "    # 1. Define all columns needed: Features + Target + Metadata\n",
    "    all_cols = REQUIRED_RAW_FEATURES + [\"MADE_PLAYOFFS\"]\n",
    "    meta_cols = [c for c in [\"SEASON\", \"TEAM_ID\", \"TEAM_NAME\"] if c in df.columns]\n",
    "\n",
    "    # 2. Create the working DataFrame with only necessary columns\n",
    "    working_df = df[all_cols + meta_cols].copy()\n",
    "    \n",
    "    # 3. Handle NaN Alignment: Drop rows where any REQUIRED_RAW_FEATURE is NaN\n",
    "    rows_before_drop = working_df.shape[0]\n",
    "    \n",
    "    # ðŸ›‘ CRITICAL FIX: Filtering the single DataFrame ensures X and Y stay perfectly aligned.\n",
    "    working_df.dropna(subset=REQUIRED_RAW_FEATURES, inplace=True)\n",
    "    \n",
    "    rows_after_drop = working_df.shape[0]\n",
    "    if rows_before_drop != rows_after_drop:\n",
    "        print(f\"WARNING: Dropped {rows_before_drop - rows_after_drop} rows with NaN values during feature cleanup.\")\n",
    "    \n",
    "    # 4. Final selection and cleaning\n",
    "    X_clean = working_df[REQUIRED_RAW_FEATURES].reset_index(drop=True)\n",
    "    \n",
    "    # ðŸ›‘ Target selection and cleaning (ensuring integer type)\n",
    "    y_clean = working_df[\"MADE_PLAYOFFS\"].reset_index(drop=True).astype(int)\n",
    "    \n",
    "    # Metadata\n",
    "    meta_clean = working_df[meta_cols].reset_index(drop=True)\n",
    "    \n",
    "    print(f\"Features Selected ({len(X_clean.columns)}): {list(X_clean.columns)}\")\n",
    "    print(f\"Final Data Shapes: X={X_clean.shape}, y={y_clean.shape}\")\n",
    "\n",
    "    # 5. Final check\n",
    "    if len(X_clean.columns) != 13:\n",
    "        raise ValueError(f\"Feature selection failed. Expected 13 columns, found {len(X_clean.columns)}.\")\n",
    "    \n",
    "    # The return order ensures X, y, meta matches the main block\n",
    "    return X_clean, y_clean, meta_clean\n",
    "\n",
    "def train_forest(X, y):\n",
    "    \"\"\"\n",
    "    Trains a Random Forest Classifier with stratified splitting.\n",
    "    \"\"\"\n",
    "    # Stratify ensures the train/test split has the same % of playoff teams\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=0.2, random_state=42, stratify=y\n",
    "    )\n",
    "    \n",
    "    # Initialize Model\n",
    "    clf = RandomForestClassifier(\n",
    "        n_estimators=200,      # More trees for stability\n",
    "        max_depth=12,          # Prevent extreme overfitting\n",
    "        min_samples_split=5,   # Conservative splitting\n",
    "        random_state=42,\n",
    "        n_jobs=-1\n",
    "    )\n",
    "    \n",
    "    print(\"Training Random Forest...\")\n",
    "    clf.fit(X_train, y_train)\n",
    "    return clf, X_train, X_test, y_train, y_test\n",
    "\n",
    "def run_tscv_metrics(model, X, y, n_splits=5):\n",
    "    \"\"\"\n",
    "    Performs Time Series Cross-Validation (TSCV) and collects \n",
    "    all 5 required metrics (Accuracy, Precision, Recall, F1, ROC-AUC) \n",
    "    for a single model.\n",
    "    \n",
    "    NOTE: X and y are expected to be pure NumPy arrays from the main block\n",
    "          to ensure clean slicing, which resolves the multilabel ValueError.\n",
    "    \"\"\"\n",
    "    # Lists to store metrics for each fold\n",
    "    # ðŸ›‘ FIX: Define the results dictionary UNCONDITIONALLY at the start\n",
    "    results = {\n",
    "        'Accuracy': [], 'Precision': [], 'Recall': [], 'F1 Score': [], 'ROC-AUC': []\n",
    "    }\n",
    "    \n",
    "    # CRITICAL FIX: Implement Scaling for Logistic Regression\n",
    "    if isinstance(model, LogisticRegression):\n",
    "        # LogReg needs scaled features (X is assumed to be a NumPy array)\n",
    "        scaler = StandardScaler()\n",
    "        X_input = scaler.fit_transform(X)\n",
    "    else:\n",
    "        # Tree-based models (RF, GB) do not need scaling\n",
    "        X_input = X # X_input is also a NumPy array here\n",
    "        \n",
    "    tscv = TimeSeriesSplit(n_splits=n_splits)\n",
    "    \n",
    "    print(f\"-> Evaluating {model.__class__.__name__} with TSCV...\")\n",
    "    \n",
    "    # Iterate through each train/test split generated by TSCV\n",
    "    for fold, (train_index, test_index) in enumerate(tscv.split(X_input)):\n",
    "        X_train, X_test = X_input[train_index], X_input[test_index]\n",
    "        y_train, y_test = y[train_index], y[test_index]\n",
    "        y_train = y_train.ravel()\n",
    "        y_test = y_test.ravel()\n",
    "        \n",
    "        # Clone the model (important to reset weights for each fold)\n",
    "        clf = clone(model)\n",
    "        clf.fit(X_train, y_train)\n",
    "        \n",
    "        y_pred = clf.predict(X_test)\n",
    "        # Get probabilities for ROC-AUC (using np.array() for robustness)\n",
    "        y_probs = np.array(clf.predict_proba(X_test))[:, 1]\n",
    "        \n",
    "        # Calculate and store metrics \n",
    "        results['Accuracy'].append(accuracy_score(y_test, y_pred))\n",
    "        \n",
    "        # ðŸ›‘ FIX: Explicitly set average='binary' and pos_label=1 to prevent ValueError\n",
    "        results['Precision'].append(precision_score(y_test, y_pred, average='binary', pos_label=1))\n",
    "        results['Recall'].append(recall_score(y_test, y_pred, average='binary', pos_label=1))\n",
    "        results['F1 Score'].append(f1_score(y_test, y_pred, average='binary', pos_label=1))\n",
    "        \n",
    "        results['ROC-AUC'].append(roc_auc_score(y_test, y_probs))\n",
    "\n",
    "    # Calculate mean and std for final table\n",
    "    final_results = {}\n",
    "    for metric, scores in results.items():\n",
    "        mean_score = np.mean(scores)\n",
    "        std_dev = np.std(scores)\n",
    "        # Format the string exactly as requested\n",
    "        final_results[metric] = f\"{mean_score:.4f} (Â±{std_dev:.4f})\"\n",
    "        \n",
    "    return final_results\n",
    "\n",
    "def print_tscv_table(results_dict):\n",
    "    \"\"\"Prints the final comparison table in a markdown format.\"\"\"\n",
    "    \n",
    "    model_names = list(results_dict.keys())\n",
    "    metrics = list(results_dict[model_names[0]].keys())\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"FINAL TIME SERIES CROSS-VALIDATION RESULTS (Mean Score Â± Std Dev)\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # Print Header\n",
    "    header = \"| Model | \" + \" | \".join([f\"Avg. {m}\" for m in metrics]) + \" |\"\n",
    "    print(header)\n",
    "    print(\"|\" + \"---|\" * (len(metrics) + 1))\n",
    "    \n",
    "    # Print Rows\n",
    "    for model_name in model_names:\n",
    "        row = f\"| {model_name} | \"\n",
    "        metric_values = [results_dict[model_name][m] for m in metrics]\n",
    "        row += \" | \".join(metric_values) + \" |\"\n",
    "        print(row)\n",
    "\n",
    "def evaluate_performance(clf, X_test, y_test):\n",
    "    \"\"\"\n",
    "    Generates comprehensive metrics and confusion matrix, including ROC-AUC.\n",
    "    \"\"\"\n",
    "    y_pred = clf.predict(X_test)\n",
    "    y_probs = clf.predict_proba(X_test)[:, 1] # Get probabilities for the positive class (1)\n",
    "    \n",
    "    # Calculate ROC-AUC\n",
    "    roc_auc = roc_auc_score(y_test, y_probs) # <-- NEW CALCULATION\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*40)\n",
    "    print(\"MODEL EVALUATION METRICS\")\n",
    "    print(\"=\"*40)\n",
    "    print(f\"Accuracy:  {accuracy_score(y_test, y_pred):.4f}\")\n",
    "    print(f\"Precision: {precision_score(y_test, y_pred, average='binary', pos_label=1):.4f} (Low False Positives)\")\n",
    "    print(f\"Recall:    {recall_score(y_test, y_pred, average='binary', pos_label=1):.4f} (Low False Negatives)\")\n",
    "    print(f\"F1 Score:  {f1_score(y_test, y_pred, average='binary', pos_label=1):.4f}\")\n",
    "    print(f\"ROC-AUC:   {roc_auc:.4f} (Area Under the Curve)\") # <-- NEW OUTPUT\n",
    "    \n",
    "    print(\"\\nClassification Report:\")\n",
    "    print(classification_report(y_test, y_pred))\n",
    "    \n",
    "    # Confusion Matrix Visualization\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=[\"Miss\", \"Make\"])\n",
    "    disp.plot(cmap=\"Blues\")\n",
    "    plt.title(\"Confusion Matrix: Playoff Prediction\")\n",
    "    plt.savefig(os.path.join(VIS_DIR, \"confusion_matrix.png\"))\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "def analyze_random_forest_comparison_plot(X_train, y_train, BRONZE_DIR):\n",
    "    \"\"\"\n",
    "    Trains a final Random Forest model and generates a side-by-side plot\n",
    "    of Mean Decrease in Impurity (MDI) and Permutation Importance (AUC Drop).\n",
    "    \"\"\"\n",
    "    # Define the Random Forest Model (using parameters from your previous runs)\n",
    "    rf_model = RandomForestClassifier(\n",
    "        n_estimators=200, max_depth=12, min_samples_split=5, \n",
    "        random_state=42, n_jobs=-1\n",
    "    )\n",
    "    \n",
    "    print(\"Training final Random Forest model for feature comparison plot...\")\n",
    "    rf_model.fit(X_train, y_train)\n",
    "\n",
    "    # 1. Calculate Permutation Importance (AUC Drop)\n",
    "    print(\"Calculating Permutation Importance (AUC Drop)...\")\n",
    "    perm_result = permutation_importance(\n",
    "        rf_model, X_train, y_train, \n",
    "        n_repeats=10, random_state=42, n_jobs=-1, scoring='roc_auc'\n",
    "    )\n",
    "    perm_importances = perm_result.importances_mean\n",
    "    \n",
    "    # 2. Get Mean Decrease in Impurity (MDI)\n",
    "    mdi_importances = rf_model.feature_importances_\n",
    "    feature_names = X_train.columns\n",
    "    \n",
    "    # Create DataFrames\n",
    "    mdi_df = pd.DataFrame({'Feature': feature_names, 'Importance': mdi_importances}).sort_values(by='Importance', ascending=False)\n",
    "    perm_df = pd.DataFrame({'Feature': feature_names, 'Importance': perm_importances}).sort_values(by='Importance', ascending=False)\n",
    "    \n",
    "    # 3. Visualization: Side-by-Side Comparison \n",
    "    fig, axes = plt.subplots(1, 2, figsize=(18, 10))\n",
    "    \n",
    "    # MDI Plot\n",
    "    sns.barplot(ax=axes[0], x='Importance', y='Feature', data=mdi_df, palette=\"viridis\")\n",
    "    axes[0].set_title('Mean Decrease in Impurity (MDI)')\n",
    "    axes[0].set_xlabel('Importance')\n",
    "    \n",
    "    # Permutation Plot\n",
    "    sns.barplot(ax=axes[1], x='Importance', y='Feature', data=perm_df, palette=\"magma\")\n",
    "    axes[1].set_title('Permutation Importance (AUC Drop)')\n",
    "    axes[1].set_xlabel('Mean AUC Drop')\n",
    "\n",
    "    plt.suptitle(\"Random Forest Feature Importance Comparison (13 Ranks)\")\n",
    "    plt.tight_layout(rect=[0, 0, 1, 0.96]) # Adjust layout for suptitle\n",
    "    \n",
    "    save_path = os.path.join(BRONZE_DIR, \"visualizations\", \"random_forest_mdi_perm_comparison.png\")\n",
    "    plt.savefig(save_path)\n",
    "    plt.close()\n",
    "    print(f\"âœ… Random Forest Comparison visualization saved to {save_path}\")\n",
    "\n",
    "    return rf_model\n",
    "\n",
    "def analyze_logreg_coefficients(X_train, y_train, BRONZE_DIR):\n",
    "    \"\"\"\n",
    "    Trains the final Logistic Regression model (via Pipeline) and visualizes its coefficients.\n",
    "    \"\"\"\n",
    "    logreg_pipeline = Pipeline([\n",
    "        ('scaler', StandardScaler()),\n",
    "        ('logreg', LogisticRegression(solver='liblinear', random_state=42))\n",
    "    ])\n",
    "    \n",
    "    print(\"Training final Logistic Regression model for coefficient analysis...\")\n",
    "    logreg_pipeline.fit(X_train, y_train)\n",
    "    \n",
    "    coefficients = logreg_pipeline['logreg'].coef_[0]\n",
    "    feature_names = X_train.columns\n",
    "    \n",
    "    # Create a DataFrame for easy sorting and plotting\n",
    "    coef_df = pd.DataFrame({\n",
    "        'Feature': feature_names,\n",
    "        'Coefficient': coefficients\n",
    "    }).sort_values(by='Coefficient', ascending=False)\n",
    "    \n",
    "    # Visualization \n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.barplot(x='Coefficient', y='Feature', data=coef_df, palette=\"coolwarm\")\n",
    "    plt.title(\"Logistic Regression Coefficients (Impact on Playoff Odds)\")\n",
    "    plt.xlabel(\"Coefficient Value (Standardized)\")\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    save_path = os.path.join(BRONZE_DIR, \"visualizations\", \"logreg_coefficients.png\")\n",
    "    plt.savefig(save_path)\n",
    "    plt.close()\n",
    "    print(f\"âœ… Logistic Regression Coefficient visualization saved to {save_path}\")\n",
    "    \n",
    "    return logreg_pipeline # Return the pipeline\n",
    "\n",
    "def persist_model(clf):\n",
    "    \"\"\"\n",
    "    Saves the model using Joblib.\n",
    "    \"\"\"\n",
    "    print(f\"Saving model to {MODEL_OUTPUT}...\")\n",
    "    joblib.dump(clf, MODEL_OUTPUT, compress=3)\n",
    "    print(\"Model serialized successfully.\")\n",
    "\n",
    "\n",
    "def plot_roc_curve(clf, X_test, y_test, model_name, BRONZE_DIR):\n",
    "    \"\"\"\n",
    "    Generates and saves the ROC-AUC curve for the final model.\n",
    "    \"\"\"\n",
    "    # 1. Calculate the AUC score\n",
    "    y_probs = clf.predict_proba(X_test)[:, 1]\n",
    "    auc_score = roc_auc_score(y_test, y_probs)\n",
    "    \n",
    "    # 2. Plot the ROC Curve\n",
    "    plt.figure(figsize=(8, 8))\n",
    "    roc_display = RocCurveDisplay.from_estimator(\n",
    "        clf, X_test, y_test, \n",
    "        name=f'{model_name} (AUC = {auc_score:.4f})'\n",
    "    )\n",
    "    \n",
    "    # 3. Add formatting and the random guess line\n",
    "    plt.plot([0, 1], [0, 1], 'k--', label='Random Guess (AUC = 0.5)')\n",
    "    plt.title(f'ROC Curve for {model_name} Playoff Prediction')\n",
    "    plt.xlabel('False Positive Rate (FPR)')\n",
    "    plt.ylabel('True Positive Rate (TPR)')\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    plt.grid(True)\n",
    "    \n",
    "    # 4. Save the plot\n",
    "    save_path = os.path.join(BRONZE_DIR, \"visualizations\", f\"{model_name.lower().replace(' ', '_')}_roc_curve.png\")\n",
    "    plt.savefig(save_path)\n",
    "    plt.close()\n",
    "    print(f\"âœ… {model_name} ROC Curve visualization saved to {save_path}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # 1. Load and Sanitize Data\n",
    "    # X_pd and y_pd are the cleaned Pandas DataFrame/Series of the same length (N rows)\n",
    "    X_pd, y_pd, meta = load_and_sanitize(INPUT_FILE)\n",
    "    \n",
    "    # ðŸ›‘ CRITICAL FIX: Convert to pure NumPy arrays for reliable slicing in TSCV.\n",
    "    # We use .ravel() on y to guarantee a clean 1-dimensional vector (N,) shape.\n",
    "    X = X_pd.values\n",
    "    y = y_pd.values.ravel()\n",
    "    \n",
    "    print(\"-\" * 50)\n",
    "    print(f\"Loaded Data Shapes - X: {X.shape}, y: {y.shape}\")\n",
    "    if X.shape[0] != y.shape[0]:\n",
    "        # This check should now only fail if load_and_sanitize returned mismatched lengths\n",
    "        print(\"CRITICAL ERROR: X and Y have inconsistent lengths!\")\n",
    "        raise ValueError(f\"X ({X.shape[0]} samples) and Y ({y.shape[0]} samples) have inconsistent lengths after cleaning.\")\n",
    "    print(\"-\" * 50)\n",
    "\n",
    "\n",
    "    # Define the models to test for the TSCV comparison\n",
    "    models_to_test = {\n",
    "        \"RandomForest\": RandomForestClassifier(n_estimators=200, max_depth=12, min_samples_split=5, random_state=42, n_jobs=-1),\n",
    "        \"GradientBoosting\": GradientBoostingClassifier(random_state=42),\n",
    "        \"LogisticRegression\": LogisticRegression(solver='liblinear', random_state=42)\n",
    "    }\n",
    "    \n",
    "    all_results = {}\n",
    "    \n",
    "    # 2. PERFORM TIME SERIES CROSS-VALIDATION (TSCV)\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(f\"STARTING TIME SERIES CROSS-VALIDATION (k=5 folds)\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    for name, model in models_to_test.items():\n",
    "        # X and y are the NumPy arrays\n",
    "        all_results[name] = run_tscv_metrics(model, X, y)\n",
    "        \n",
    "    # 3. PRINT THE FINAL COMPARISON TABLE\n",
    "    print_tscv_table(all_results)\n",
    "    \n",
    "    # 4. Split the data for final training/testing (using the NumPy arrays X and y)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=0.2, random_state=42, stratify=y\n",
    "    )\n",
    "    \n",
    "    # 5. RUN BOTH FEATURE IMPORTANCE ANALYSES\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"FINAL FEATURE IMPORTANCE COMPARISON\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    # CONVERSION: Convert NumPy arrays back to Pandas DataFrames for visualization functions\n",
    "    # This is necessary so the plot labels (feature names) are correctly carried over.\n",
    "    X_train_pd = pd.DataFrame(X_train, columns=X_pd.columns)\n",
    "    X_test_pd = pd.DataFrame(X_test, columns=X_pd.columns)\n",
    "    \n",
    "    # Analyze with Random Forest\n",
    "    _ = analyze_random_forest_comparison_plot(X_train_pd, y_train, BRONZE_DIR) \n",
    "\n",
    "    # Analyze with the BEST model (Logistic Regression)\n",
    "    final_model_pipeline = analyze_logreg_coefficients(X_train_pd, y_train, BRONZE_DIR)\n",
    "    \n",
    "    # 6. Final Evaluation and Persistence (using the BEST model: LogReg)\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"FINAL LOGISTIC REGRESSION TEST SET EVALUATION\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    # Use the Pandas test set here because the final_model_pipeline (LogReg) uses a StandardScaler\n",
    "    # which performs best when consistently trained/tested on DataFrames (though NumPy is often fine).\n",
    "    evaluate_performance(final_model_pipeline, X_test_pd, y_test)\n",
    "    \n",
    "    plot_roc_curve(final_model_pipeline, X_test_pd, y_test, \"Logistic Regression\", BRONZE_DIR)\n",
    "    \n",
    "    persist_model(final_model_pipeline)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
